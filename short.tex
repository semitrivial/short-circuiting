\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathrsfs}
% \smartqed  % flush right qed marks, e.g. at end of proof
% \usepackage{graphicx}

\newcommand{\myclaim}[1]{\textbf{Claim #1:}}
\def\myulcorner{\mathord{\ulcorner}}
\def\myurcorner{\mathord{\urcorner}}

\pagenumbering{gobble}

\begin{document}

\title{Short-circuiting the definition of mathematical knowledge for an
Artificial General Intelligence
%\thanks{}
}

\titlerunning{Short-circuiting the definition of mathematical knowledge for an AGI}

\author{Samuel Allen
Alexander\inst{1}\orcidID{0000-0002-7930-110X}}

\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}


\maketitle

\begin{abstract}
We propose that, for the purpose of studying theoretical properties of
the knowledge of an agent with Artificial General Intelligence (that is, 
the knowledge of an AGI),
a pragmatic way to define such an agent's knowledge (in the language
of Epistemic Arithmetic (EA)) is as follows.
We declare an AGI to know a certain EA-statement $\phi$
if and only if that AGI would include $\phi$ in the resulting
enumeration if that AGI were commanded:
``Enumerate all the EA-sentences which you know.''
This definition is non-circular because an AGI, being capable of
practical English communication, is capable of understanding the everyday
English word ``know'' independently of how any philosopher formally
defines knowledge (we elaborate further on the non-circularity of this
circular-looking definition).
This elegantly solves the problem that different AGIs may have
very different internal knowledge definitions and yet we want
to be able to study knowledge of AGIs in general, without having
to study different AGIs separately just because they have separate
internal knowledge definitions. Finally, we suggest how this
definition of AGI knowledge can be used as a bridge which could
allow the AGI research community to import certain abstract results
about mechanical knowing agents from mathematical logic.
\keywords{AGI \and machine knowledge}
\end{abstract}

\section{Introduction}

It is difficult to define knowledge, or what it means to know something.
In Plato's dialogs, again and again Socrates asks people to define
knowledge\footnote{Perhaps the best example being in the \emph{Theaetetus}
\cite{theaetetus}.}, and no-one ever succeeds. Neither have philosophers
reached consensus even in our own era \cite{sep-knowledge-analysis}.

At the same time, the problem is often brushed aside as something only
philosophers care about: pragmatists rarely spend time
on this sort of debate. The exception is the area of Artificial General Intelligence
(AGI), where even the staunchest pragmatists admit the importance of the question.

In this paper, we narrow down the question ``what is knowledge'' and offer
a simple answer within that narrow context:
we propose a definition of what it means for a suitably idealized AGI to know
a mathematical sentence\footnote{By a \emph{sentence}, we mean a formula with
no free variables. Thus, $x^2>0$ is not a sentence, but
$\forall x (x^2>0)$ is.} in the language of Epistemic Arithmetic \cite{shapiro} (hereafter
EA; EA
consists of
the language of Peano Arithmetic along with an additional modal operator $K$ for
knowledge---for example, the EA-sentence $K(1+1=2)$ might be read ``I know $1+1=2$''
or ``the knower knows $1+1=2$''). Our
proposed definition is short and
sweet (at the price of appearing deceptively circular): we say that
an AGI knows an EA-sentence $\phi$
if and only if $\phi$ would be among the sentences which that AGI would
enumerate if that AGI were commanded:
\[
\text{``Enumerate all the EA-sentences which you know.''}
\]
This is non-circular because an AGI, being capable of practical English
communication, is therefore capable of understanding the everyday English word
``know'' in the above command, independently of how any philosopher formally
defines knowledge. We discuss this further in Subsection \ref{noncircularsubsection}.

Our proposed definition is not directly intended
for methodological purposes---it would not
directly be helpful in the quest to construct an AGI. Instead, it is intended for
the purpose of understanding the properties of AGI (some of which
we discuss in Section \ref{appsection}). We hope that a better
understanding of theoretical properties of AGIs will indirectly help in their
creation someday.

The structure of this paper is as follows.
\begin{itemize}
  \item In Section \ref{agisection} we discuss the AGIs for whose knowledge we are
  attempting to propose a definition.
  \item In Section \ref{mainsection} we propose a knowledge definition for
  AGIs for EA-sentences.
  \item In Section \ref{quantifiedsection} we extend our knowledge definition
  to formulas with free variables.
  \item In Section \ref{appsection} we use this knowledge definition as a bridge
  to translate some ideas from mathematical logic into the field of AGI.
  \item In Section \ref{conclusionsection} we summarize and make concluding remarks.
\end{itemize}

\section{Idealized AGIs}
\label{agisection}

In this paper, we approach AGI using what
Goertzel \cite{goertzel2014artificial} calls
the Universalist Approach:
we adopt ``...an idealized case of AGI, similar to
assumptions like the frictionless plane in physics'', hoping that by
understanding this ``simplified special
case, we can use the understanding we've gained to address more realistic
cases.''

We do not have a formal definition for what an AGI is, but whatever it is,
we assume an AGI is a deterministic machine
which repeatedly reads sensory input from its environment and outputs
English words based on what sensory inputs it has received so far.
When we say that this AGI is a ``deterministic machine'', we mean that
said outputs (considered as a function of said inputs) could be computed
by a Turing machine. We further assume the AGI can understand English
commands and is capable of practical English communication. Thus, if
we were to command the AGI in English, ``Tell us the value of $1+1$'',
the AGI would respond in English and reply ``$2$'', or ``$1+1=2$'',
or something along those lines.

We assume an AGI is capable of everyday English discussions
which would cause no difficulty to a casual English speaker, even if
these discussions involve topics, such as ``knowledge'', which
might be philosophically tricky. A casual English speaker does not get
bogged down in philosophical questions about the nature of knowledge
just in order to answer a question like ``Do you know that $1+1=2$?'',
and therefore neither should our AGI.

We also assume an AGI is better than a casual English speaker in certain ways.
We assume an AGI would have no objections to performing exceedingly tedious
tasks indefinitely, if so commanded. If we asked a casual English speaker to
begin computing and reciting all the prime numbers until further notice, and
then we waited silently forever listening to the results, said human would eventually
get tired of the endless tedium and would disobey our command (not to mention
the fact that they might make arithmetic errors along the way). We assume an AGI
has no such limitations and would happily compute and recite prime numbers for
all eternity, if so commanded---and would make no arithmetic mistakes. Of course,
in reality the AGI would eventually run out of memory, or perish in the heat
death of the universe, etc., but we are speaking of idealized AGI here and we
intentionally ignore such possibilities, in the same way a Turing machine is
assumed to have infinite tape and infinite time to run.


\section{An elegant definition of mathematical knowledge}
\label{mainsection}

The following definition might initially look circular, but we will argue that
it is not.

\begin{definition}
\label{maindef}
  Let $X$ be an AGI.
  For any EA-sentence $\phi$, we say that $X$
  knows $\phi$ if and only
  if $X$ would eventually include $\phi$ in the resulting enumeration
  if $X$ were commanded:
  \[
  \text{``Enumerate all the EA-sentences which you know.''}
  \]
\end{definition}

Definition \ref{maindef} is non-circular because
the AGI is capable (see Section \ref{agisection}) of practical English
communication, including that involving everyday English words such as
the word ``know'', independently of how any philosophers formally
define things. More on this in Subsection \ref{noncircularsubsection}.


One of the strengths of Definition \ref{maindef} is that it is uniform across
different AGIs: many different AGIs might internally operate based on different
definitions of knowledge, but Definition \ref{maindef} works equally well for
all these different AGIs regardless of those different internal knowledge
definitions\footnote{This is reminiscent of Elton's proposal that instead of
trying to interpret an AI's outputs by focusing on specific low-level details
of a neural network, we should instead let the AI explain itself \cite{elton}.}.

Although Definition \ref{maindef} may differ significantly from a particular AGI
$X$'s own internal definition of knowledge, the following theorem states that
materially the two definitions have the same result.

\begin{theorem}
\label{sentenceequivalence}
  Suppose $X$ is an AGI. For any EA-sentence $\phi$, the following
  are equivalent:
  \begin{enumerate}
    \item $X$ is
    considered to know $\phi$ (based on Definition \ref{maindef}).
    \item
    $X$ knows $\phi$ (based on $X$'s own internal understanding of
    knowledge).
  \end{enumerate}
\end{theorem}

\begin{proof}
  By Definition \ref{maindef}, (1) is equivalent to the statement that $X$ would
  include $\phi$ in the list which $X$ would output if $X$ were commanded:
  \[
  \text{``Enumerate all the EA-sentences which you know.''}
  \]
  Since we have assumed (in Section \ref{agisection}) that $X$ is obedient,
  $X$ would output
  $\phi$ in the resulting list if and only if (2).
  \qed
\end{proof}

\begin{theorem}
\label{retheorem}
  Let $X$ be an AGI.
  The set of EA-sentences $\phi$ such that $X$ knows $\phi$ (based on
  Definition \ref{maindef}) is computably enumerable.
\end{theorem}

\begin{proof}
  This follows from our assumption (in Section \ref{agisection}) that
  $X$ is a deterministic machine.
  \qed
\end{proof}


\subsection{Non-Circularity of Definition \ref{maindef}}
\label{noncircularsubsection}

\begin{quote}
  `What is said by a speaker (what she meant to say, her ``meaning-intention'')
  is understood or misunderstood by a
  hearer (``an interpreter'').'
  ---Albrecht Wellmer \cite{wellmer2014skepticism}
\end{quote}

Definition \ref{maindef} is non-circular because an AGI's response to an English
command only depends on how the AGI understands the words in that command, not
on how \emph{we} (the speakers) understand those words.
Recall from Section \ref{agisection}
that we are assuming an AGI is a deterministic machine which outputs English words
based on sensory inputs from its environment. Those outputs depend \emph{only} on those
environmental inputs, and not on any decisions made by philosophers.

If the reader wants to further convince themselves of the non-circularity of Definition
\ref{maindef}, we need only point out that the apparent circularity would disappear if
we changed Definition \ref{maindef} to define what it means for $X$ to ``grok'' sentence
$\phi$, rather that to ``know'' sentence $\phi$ (without changing the command itself).
In other words, we could define that $X$ ``groks'' $\phi$ if and only if $X$ would include
$\phi$ in the list of sentences that would result if $X$ were commanded,
  \[
  \text{``Enumerate all the EA-sentences which you know.''}
  \]
This would make the non-circularity clearer, because the word ``grok'' does not appear
anywhere in the definition.

For the sake of completion, we will further illustrate the non-circularity of
Definition \ref{maindef} with two examples.
\begin{itemize}
  \item
  (The color blurple) Bob could (without Alice's awareness) define ``blurple'' to be the
  color of the card
  which Alice would choose if Bob were to run up to Alice, present her a red card
  and a blue card, and demand: ``Quick, choose the blurple card! Do it now, no time
  for questions!'' There is nothing circular about this, because Alice's choice
  cannot depend on a definition which Alice is unaware of.
  \item
  (Zero to the zero) If asked to compute $0^0$, some calculators output $1$, and some
  output an error message or say the result is undefined\footnote{Which is incorrect---see
  \cite{knuth}.}. For any calculator $X$, it
  would be perfectly non-circular to define ``the $0^0$ of $X$'' to be the output which
  $X$ outputs when asked to compute $0^0$. Said output is pre-programmed into the
  calculator; the calculator does not read the user's mind in order to base its answer
  on any definitions that exist there.
\end{itemize}

\subsection{Sentences using the Knowledge Operator}

Definition \ref{maindef} is particularly interesting when $\phi$ itself
contains makes use of EA's $K$ operator for knowledge.

\begin{example}
Applying Definition \ref{maindef},
we consider an AGI $X$ to know $K(1+1=2)$ if and only if that AGI would output
$K(1+1=2)$ when commanded to enumerate all the EA-sentences he knows.
$X$ would (when so commanded)
output $K(1+1=2)$ if and only if $X$ knows (in his own internal sense of the word ``know'')
that he knows (in his own internal sense of the word ``know'') $1+1=2$.
\end{example}

\subsection{A Simpler Definition, and Why It Does Not Work}

\begin{quote}
  ``It is difficult to be aware of whether one knows or not.
  For it is difficult to be aware of whether we know from the
  principles of a thing or not---and that is what knowing is.
  (...) I call a principle in each genus those which it is
  not possible to prove to be.'' ---Aristotle \cite{aristotle}
\end{quote}

The reader might wonder why we would not further simplify Definition \ref{maindef}
and declare that $X$ knows $\phi$ if and only if $X$ would respond ``yes'' if
$X$ were asked: ``Do you know $\phi$? (Yes or no)''.
We will argue that this would be a poor candidate for an idealized knowledge
definition.

\begin{definition}
\label{instantlyknowsdefn}
  If $X$ is an AGI and $\phi$ is an EA-sentence, say that $X$ \emph{quick-knows}
  $\phi$ if and only if $X$ would respond ``yes'' if $X$ were asked,
  ``Do you know $\phi$? (Yes or no)''.
\end{definition}

The following should be contrasted with Theorem \ref{retheorem}.

\begin{theorem}
\label{computabletheorem}
  Let $X$ be an AGI.
  The set of EA-sentences $\phi$ such that $X$ quick-knows $\phi$ is computable.
\end{theorem}

\begin{proof}
  This follows from our assumption (in Section \ref{agisection}) that
  $X$ is a deterministic machine.
  \qed
\end{proof}

By Theorem \ref{computabletheorem}, it seems that if we used
Definition \ref{instantlyknowsdefn} as a knowledge definition, it would
contradict Aristotle's claim that
``it is difficult to be aware of whether one
knows or not''. It is much more plausible that knowledge be \emph{computably enumerable}
(as in Theorem \ref{retheorem}) than that knowledge be \emph{computable}.
A prototypical example of a process which is computably enumerable but not computable
would be: enumerating the consequences of Peano arithmetic\footnote{We assume Peano
arithmetic is true.} (hereafter PA).
Said consequences cannot be computable, lest they could be used to solve the Halting
Problem (because a Turing machine halts if and only if PA proves that it halts).

\begin{theorem}
\label{badnesstheorem}
Let $X$ be an AGI and assume $X$ does not quick-know any falsehoods.
At least one of the following is true:
\begin{enumerate}
  \item There is an axiom of PA which $X$ does not quick-know.
  \item There exist PA-sentences $\phi$ and $\psi$
  such that $X$ quick-knows $\psi$ and $X$ quick-knows $\psi\rightarrow\phi$,
  but $X$ does not quick-know $\phi$.
\end{enumerate}
\end{theorem}

\begin{proof}
  It is well-known that a sentence $\phi$ is provable from PA if and only
  if there is a sequence $\phi_1,\ldots,\phi_n$ such that:
  \begin{enumerate}
    \item $\phi_n$ is $\phi$.
    \item For every $i$, either $\phi_i$ is an axiom of PA, or else there
    are $j,k<i$ such that $\phi_k$ is $\phi_j\rightarrow \phi_i$.
  \end{enumerate}
  (Loosely speaking: proofs from PA can be carried out using no rule
  of inference besides Modus Ponens.)
  For any formula $\phi$ which PA proves, let $|\phi|$ be the smallest
  $n$ such that there is a sequence $\phi_1,\ldots,\phi_n$ as above.

  Call a PA-sentence $\phi$ \emph{elusive} if PA proves $\phi$ but $X$ does not
  quick-know $\phi$. By Theorem \ref{computabletheorem}, the fact that $X$ does
  not quick-know any falsehoods, and the unsolvability of the Halting Problem,
  it follows that some elusive $\phi$ exists---otherwise, to computably determine whether
  or not a given Turing machine $M$ halts, we could simply ask $X$, ``Do you know
  Turing machine $M$ halts? (Yes or no)''.

  Since some elusive $\phi$ exists, there exists an elusive $\phi$ such that $|\phi|$
  is as small as possible---that is, such that $|\phi|\leq |\psi|$ for every
  elusive $\psi$. Fix such a $\phi$.

  Case 1: $\phi$ is an axiom of PA. Then condition (1) of the theorem is
  satisfied, as desired.

  Case 2: $\phi$ is not an axiom of PA.
  Let $\phi_1,\ldots,\phi_{|\phi|}$ be as in the first paragraph of this proof
  (so $\phi_{|\phi|}$ is $\phi$).
  Then since $\phi$ is not an axiom of PA,
  there must be $j,k<|\phi|$ such that $\phi_k$ is $\phi_j\rightarrow \phi_{|\phi|}$.
  Now, the sequence $\phi_1,\ldots,\phi_k$ witnesses that PA
  proves $\phi_k$ and $|\phi_k|\leq k<|\phi|$; and the
  sequence $\phi_1,\ldots,\phi_j$ witnesses that PA proves
  $\phi_j$ and $|\phi_j|\leq j<|\phi|$.
  Thus, since $\phi$ was chosen to be elusive with $|\phi|$ as small as possible,
  it follows that $\phi_k$ and $\phi_j$ are not elusive.
  Thus, $X$ quick-knows $\phi_j$, and $X$ quick-knows $\phi_k$,
  but $\phi_k$ is $\phi_j\rightarrow\phi$. Thus condition (2) of the theorem is
  satisfied, as desired.
  \qed
\end{proof}

Theorem \ref{badnesstheorem} clearly shows that Definition \ref{instantlyknowsdefn}
makes a terrible notion of idealized knowledge. An AGI should most certainly
know the axioms of PA, and should most certainly be capable of the
minimal logical reasoning needed to conclude $\phi$ from $\psi$ and $\psi\rightarrow\phi$.
And the way we have established the unsuitability of Definition \ref{instantlyknowsdefn}
is nicely anticipated by the words of Aristotle quoted at the beginning of this subsection,
where the philosopher seems to explicitly identify knowledge with
knowledge from ``principles'' (which we would translate as ``axioms'').



\section{Quantified Modal Logic}
\label{quantifiedsection}

Definition \ref{maindef} only addresses sentences with no free variables.
In this section, we will extend Definition \ref{maindef} to formulas which possibly include
free variables. We are essentially adapting a trick from
Carlson \cite{carlson}.

\begin{definition}
  We define so-called \emph{numerals}, which
  are EA-terms, one numeral $\overline n$ for each natural number $n\in\mathbb N$,
  by induction: $\overline 0$ is defined to be $0$ (the constant symbol for zero from
  PA) and
  for every $n\in\mathbb N$, $\overline{n+1}$ is defined to be $S(\overline n)$
  (where $S$ is the successor symbol from PA).
\end{definition}

For example, the numeral $\overline 3$ is the term $S(S(S(0)))$.

\begin{definition}
\label{superscript_s_defn}
  If $\phi$ is an EA-formula (with free variables
  $x_1,\ldots,x_k$),
  and if $s$ is an assignment mapping variables to natural numbers, then we define $\phi^s$
  to be the sentence
  \[
    \phi(x_1|\overline{s(x_1)})(x_2|\overline{s(x_2)})\cdots (x_k|\overline{s(x_k)})
  \]
  obtained by substituting for each free variable $x_i$ the numeral $\overline{s(x_i)}$
  for $x_i$'s value according to $s$.
\end{definition}

\begin{example}
  Suppose $s(x)=0$, $s(y)=1$, and $s(z)=3$. Then
  \[
  ((z>y+x) \wedge \forall x(K(z>y+x-x)))^s
  \]
  is defined to be
  \[
  ((\overline 3 > \overline 1+\overline 0)
  \wedge \forall x( K( \overline 3 > \overline 1 + x - x ) ))
  \]
  (note that the numeral is not substituted for the later occurrences of $x$ because
  these are bound by the $\forall x$ quantifier).
\end{example}

\begin{definition}
\label{maindefextension}
  If $\phi$ is any $\mathscr L$-formula,
  and $s$ is any assignment mapping variables to $\mathbb N$,
  we say that $X$ knows $\phi$ (with variables interpreted by $s$) if and only if
  $X$ knows $\phi^s$ according to Definition \ref{maindef}.
\end{definition}

Armed with Definition \ref{maindefextension}, the Tarskian notion
\cite{sep-tarski-truth} of
truth can be extended to a complete semantics for
knowledge in EA.

\begin{example}
  Assume an AGI $X$ is clear from context.
  Suppose $\phi$ is an EA-formula, of one free variable $x$,
  which expresses ``the $x$th Turing machine eventually halts''. Suppose we want to
  assign a truth value to the formula
  \[
  \exists x (\neg K(\phi)\wedge \neg K(\neg\phi)).
  \]
  We proceed as follows.
  \begin{itemize}
  \item Following Tarski, we should declare $\exists x (\neg K(\phi)\wedge\neg K(\neg\phi))$
  is true if and only if for every assignment $s$ mapping variables to $\mathbb N$,
  $\exists x (\neg K(\phi)\wedge\neg K(\neg\phi))$ is true (with variables interpreted
  by $s$).
  \item
  By the semantics of $\exists$, the above is true if and only if
  for every assignment $s$, there is some $n\in\mathbb N$ such that
  $\neg K(\phi)\wedge\neg K(\neg\phi)$ is true (with variables interpreted by $s(x|n)$),
  where $s(x|n)$ is the assignment that agrees with $s$ except for mapping $x$ to $n$.
  \item
  By Definition \ref{maindefextension}, this is the case if and only if
  for every assignment $s$ there is
  some $n\in\mathbb N$ such that
  $X$ does not know $\phi^{s(x|n)}$ (according to Definition \ref{maindef})
  and $X$ does not know $\neg\phi^{s(x|n)}$ (according to Definition \ref{maindef}).
  \item
  By Definition \ref{superscript_s_defn} and the fact that $x$ is the only free
  variable in $\phi$, the above is the case if and only if there
  is some $n\in\mathbb N$ such that
  $X$ does not know $\phi(x|\overline n)$ (according to Definition \ref{maindef})
  and $X$ does not know $\neg\phi(x|\overline n)$ (according to Definition \ref{maindef}).
  \end{itemize}
  So ultimately, we consider $\exists x (\neg K(\phi)\wedge \neg K(\neg\phi))$
  to be true if and only if there is some $n\in \mathbb N$ such that,
  in response to the command
  ``Enumerate all the EA-sentences which you know'',
  $X$ would not include $\phi(x|\overline n)$ nor $\neg\phi(x|\overline n)$
  in the resulting enumeration.
\end{example}


\section{Knowledge formulas}
\label{appsection}

In this section, we will look at some formulas about knowledge and interpret them in the
context of AGI in terms of Definitions \ref{maindef} and \ref{maindefextension}.

\begin{example}
  (Basic axioms of knowledge) The following axiom schemas, in the language
  of EA, are taken from Carlson \cite{carlson}
  (we restrict them to sentences for purposes of simplicity).
  \begin{itemize}
    \item (E1) $K(\phi)$ whenever $\phi$ is valid (i.e., a tautology).
    Interpreted for an AGI $X$ using Definition \ref{maindef}, this becomes:
    ``If commanded to enumerate his knowledge in EA, $X$ will include
    all EA-tautologies in the resulting list.'' This is plausible
    because the set of tautologies in any computable language is computable,
    and an AGI should have no problem enumerating them.
    \item (E2) $K(\phi\rightarrow\psi)\rightarrow K(\phi)\rightarrow K(\psi)$.
    This becomes: ``If commanded to enumerate his knowledge in EA,
    if $X$ would include $\phi\rightarrow\psi$ and if $X$ would also include
    $\phi$, then $X$ would also include $\psi$.'' This is plausible because
    an AGI should certainly be capable of basic logical reasoning.
    \item (E3) $K(\phi)\rightarrow\phi$. This becomes: ``If commanded to enumerate
    his knowledge in EA, the resulting statements $X$ enumerates
    will be true.'' This is plausible since knowledge is widely regarded as
    having truthfulness as one of its requirements. Truthfulness is not a
    requirement in the definition proposed in this paper, but for any particular
    AGI, truthfulness is probably a requirement of that AGI's internal definition
    of knowledge. There is no need to worry about the AGI being misinformed about
    contingent facts about the physical world, because EA is a purely mathematical
    language in which no such contingent facts are expressible.
    \item (E4) $K(\phi)\rightarrow K(K(\phi))$. This becomes: ``If commanded to
    enumerate his knowledge in EA, if $X$ would list $\phi$,
    then $X$ would also list $K(\phi)$.'' This is plausible because presumably
    when $X$ enumerates $\phi$ in response to the command, $X$ should
    understand why he is enumerating $\phi$, namely because he knows $\phi$---so $X$
    should therefore know that he knows $\phi$.
  \end{itemize}
\end{example}

\begin{example}
\label{smtexample}
    (Reinhardt's strong mechanistic thesis
    \cite{reinhardt1985absolute} \cite{reinhardt1986epistemic}
    \cite{carlson}) Reinhardt suggested the
    EA-schema
    \[\exists e \forall x ( K(\phi) \leftrightarrow x\in W_e)\]
    as a formalization of the mechanicalness of the knower. Here, $W_e$
    is the $e$th computably enumerable set of natural numbers ($W_e$ can also
    be thought of as the set of naturals enumerated by the $e$th Turing machine).
    For simplicity, consider the case where $x$ is the lone free variable
    of $\phi$. Then in terms of Definition \ref{maindefextension}, the schema
    becomes:
    ``If $X$ were commanded to enumerate his knowledge in the language of EA,
    then the set of $n\in\mathbb N$ such that $X$ would include $\phi(x|\overline n)$
    in the resulting list, would be computably enumerable.''
    This is not just plausible but obvious, since $X$ himself
    is an AGI and thus presumably a computer.
    If $\Phi$ is the above EA-schema, then the schema $K(\Phi)$ is \emph{Reinhardt's
    strong mechanistic thesis}. Reinhardt conjectured that his strong mechanistic
    thesis is consistent with basic axioms about knowledge (i.e., that it is
    possible for a knowing machine to know that it is a machine).
    This conjecture was proved by Carlson \cite{carlson} using sophisticated
    structural results about the ordinals \cite{carlson1999}.
    See \cite{alexander2015fast} for an elementary proof of a weaker
    version of the conjecture.
\end{example}

\begin{example}
\label{reinhardtnegativeexample}
  (Reinhardt's absolute version of G\"odel's incompleteness theorem)
  If we vary the formula from Example \ref{smtexample} by requiring that the
  knowing know the value of $e$, we obtain:
  \[
    \exists e K(\forall x ( K(\phi) \leftrightarrow x\in W_e)).
  \]
  Carlson \cite{carlson} glosses this schema in English as:
  ``I am a Turing machine, and I know which one.''
  Reinhardt showed that this schema is \emph{not} consistent with basic
  axioms about knowledge. Following Carlson's gloss, this shows
  that it is impossible for an AGI to know its own code\footnote{We have
  pointed out elsewhere \cite{alexander2014machine} that Reinhardt
  implicitly assumes that the knower knows its own truthfulness, and that
  it is possible for a knowing machine to know its own code if it is allowed to
  be ignorant of its own truthfulness, despite still being truthful.
  See \cite{aldini2015self} and \cite{aldini2015theory} for some additional discussion.}.
\end{example}

\begin{example}
\label{ectexample}
  (The Epistemic Church's Thesis \cite{flagg1985church} \cite{carlson2016collapsing})
  The following EA-schema has been suggested as a kind of epistemic formalization
  of Church's Thesis:
  \[
  ( \forall x\exists y (K(\phi))  ) \rightarrow
  ( \exists e K( \forall x\exists y ( E(e,x,y) \wedge \phi  )  )  ),
  \]
  where $E(e,x,y)$ is an EA-formula which expresses that the $e$th Turing machine
  outputs $y$ on input $x$.
  For simplicity, we will just consider when $x$ and $y$ are the lone free variables
  of $\phi$.
  Then the schema becomes:
  \begin{itemize}
    \item
    ``Suppose $X$ were commanded to enumerate his EA-knowledge.
    Assume there is a (not necessarily computable) function
    $f:\mathbb N\to\mathbb N$ such that for every $n\in\mathbb N$,
    $\phi(x|\overline n)(y|\overline{f(n)})$ is \emph{individually}
    included in the resulting enumeration.
    Then in fact there is a computable
    function $f':\mathbb N\to\mathbb N$ (with Turing index $e$)
    such that the enumeration includes
    a \emph{single} statement which transcends all the aforementioned statements
    by stating all at once that $\phi(x|\overline n)(y|\overline{f'(n)})$
    is true for all $n\in\mathbb N$.''
  \end{itemize}
  This beautiful formalism seems to capture the AGI's self-reflection ability.
  We can imagine the AGI dutifully enumerating statement after statement and
  as she goes, she discovers and predicts patterns in her own enumeration.
  If the above EA-schema is called $\Phi$, then the schema $K(\Phi)$ is what is called
  the Epistemic Church's Thesis.
  Flagg proved that the Epistemic Church's Thesis is
  consistent with basic axioms of knowledge \cite{flagg1985church},
  and Carlson proved that it is also consistent with
  Reinhardt's strong mechanistic thesis \cite{carlson2016collapsing}.
\end{example}

\begin{remark}
  As far as I know, AGI has not yet received much attention in the mathematical logical
  literature. Instead, mathematical logicians tend to concern themselves with
  \emph{knowing agents} or \emph{knowing machines}. Presumably, every AGI is a
  knowing agent and a knowing machine, but certainly not every
  knowing agent (or knowing machine) is an AGI. Thus, in general,
  inconsistency results about knowing agents or knowing machines
  carry directly over to AGIs (if no knowing agent, or no knowing machine, can
  satisfy some property, then in particular no AGI can either).
  Consistency results do not generally carry over to AGIs (it may be possible for
  a knowing agent or a knowing machine to satisfy some property, but it might be
  that none of the knowing agents or knowing machines which satisfy that property
  are AGIs). Nevertheless, a consistency result about knowing agents or knowing machines
  should at least count as evidence in favor of the corresponding consistency result
  for AGIs, at least if there is no clear reason otherwise. In the examples above:
  \begin{itemize}
    \item
    Reinhardt's strong mechanistic thesis (Example \ref{smtexample})
    was proven to be consistent with basic knowledge axioms, so it is possible
    for a knowing machine to
    know that it is a machine (without necessarily knowing which machine).
    Since not every knowing machine is an AGI, it might still be impossible
    for an AGI to know it is a machine. But the consistency of Reinhardt's
    strong mechanistic thesis at least suggests evidence that an AGI can
    know it is a machine.
    \item
    Reinhardt's absolute version of the incompleteness theorem
    (Example \ref{reinhardtnegativeexample}) is an inconsitency result.
    As such, it transfers over directly to AGI, proving (at least under
    suitable idealization) that no AGI can know its own code\footnote{Or rather,
    its own code and its
    own truthfulness---we have pointed out \cite{alexander2014machine} that
    Reinhardt implicitly
    assumes the knower knows its own truthfulness.}.
    \item
    The epistemic Church's thesis (Example \ref{ectexample}) was proven to be
    consistent with basic knowledge axioms.
    Since not every knowing
    machine is an AGI, this consistency result might not hold for AGIs.
    But at least the result suggests evidence that
    an AGI can satisfy the Epistemic Church's Thesis.
  \end{itemize}
\end{remark}

\begin{example}
\label{fineexample}
  (Higher-order ignorance) Let $I(\phi)$ be shorthand for
  $\neg K(\phi)\wedge \neg K(\neg\phi)$, so $I(\phi)$ expresses the knower's
  ignorance about $\phi$. Kit Fine argued \cite{fine2018ignorance} that very
  basic assumptions
  about knowledge imply that second-order ignorance implies third-order (and hence
  all higher-order) ignorance:
  \[
    I(I(\phi)) \rightarrow I(I(I(\phi))).
  \]
  This becomes: ``If, when $X$ is commanded to enumerate his EA-knowledge,
  $X$ would include neither $I(\phi)$ nor $\neg I(\phi)$ in
  the resulting enumeration, then $X$ would also include neither $I(I(\phi))$
  nor $\neg I(I(\phi))$.''
\end{example}

\begin{example}
  (Belief and higher-order ignorance) Fano and Graziani have suggested \cite{fano} the
  following schema which would relate belief ($B$) with higher-order ignorance
  (where, as in Example \ref{fineexample}, $I(\phi)$ is shorthand for
  $\neg K(\phi)\wedge \neg K(\neg\phi)$):
  \[
    ((B(\phi) \wedge \neg\phi)\vee (B(\neg\phi)\wedge \phi)) \rightarrow I(I(\phi)),
  \]
  in other words, that incorrect belief about $\phi$ implies higher-order ignorance
  about $\phi$. It would be straightforward to give a definition similar to
  Definition \ref{maindef} for belief, along the following lines: an AGI $X$
  believes a formula $\phi$ (in a mathematical language $\mathscr L$ similar to EA but also
  including an operator $B$ for belief)
  if and only if $X$ would include $\phi$ in the resulting list if $X$ were
  commanded:
  ``Enumerate all the $\mathscr L$-sentences which you believe.''
  This proposed schema of Fano and Graziani would then become:
  ``If, when commanded to enumerate his beliefs in $\mathscr L$,
  $X$ would include $\neg\phi$ (if $\phi$ is true) or $\phi$ (if $\phi$ is false),
  then, when commanded to enumerate his knowledge in $\mathscr L$,
  $X$ would not include $I(\phi)$ nor $\neg I(\phi)$.''
\end{example}

\begin{example}
  (Intuitive Ordinal Intelligence) In \cite{ioi1} we defined
  an intelligence measure for idealized mechanical knowing agents (who are
  aware of the computable ordinals) as follows.
  If $A$ is such a knowing agent, we define the intelligence of $A$ to be
  the supremum of the set of ordinals $\alpha$ such that
  $\alpha$ has some code $c$ such that $A$ knows that $c$ is a code of
  a computable ordinal. In \cite{ioi2} we specialized this to AGIs, and
  called it \emph{Intuitive Ordinal Intelligence}.
  Let $\mathscr L$ be a language like EA but including an additional
  predicate symbol $O$ for the set of codes of computable ordinals.
  Modifying Definition \ref{maindef} accordingly,
  we can systematically perform said specialization to AGIs, and it becomes:
  ``The Intuitive Ordinal
  Intelligence of an AGI $X$ is the supremum of the set of ordinals $\alpha$
  such that there is some $c$ such that $X$ would include $O(\overline c)$
  in the resulting enumeration if we asked $X$ to enumerate all the statements
  that he knows in $\mathscr L$.''
\end{example}

\section{Conclusion}
\label{conclusionsection}

What does it mean to know something? This is a difficult question and there probably
is no one true answer. In the field of AGI, how can we systematically investigate
the theoretical properties of knowledge, when different AGIs might not even agree
about what knowledge really means? So motivated, we have proposed
an elegant way to brush these philosophical questions aside. In Definition \ref{maindef},
we declare that an AGI knows an EA-sentence if and
only if that AGI would enumerate that sentence if commanded:
  \[
  \text{``Enumerate all the EA-sentences which you know.''}
  \]
(This definition might look circular at first glance but we have argued that it is
not; see Subsection \ref{noncircularsubsection}).
In Definition \ref{maindefextension}
we extended this to formulas with free variables, not just sentences.

This one-size-fits-all knowledge definition sets the study of AGI knowledge
on a firmer theoretical footing. In Section \ref{appsection} we give examples
of how our definition can serve as a bridge to translate knowledge-related
results from mathematical logic into the realm of AGI.

\section*{Acknowledgments}

We gratefully acknowledge Alessandro Aldini, Phil Maguire, and Philippe Moser for
comments and feedback.

\bibliographystyle{splncs04}
\bibliography{short}

\end{document}